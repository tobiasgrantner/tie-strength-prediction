{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = pd.concat([pd.read_csv(f'data/Postings_{timeframe}.csv', sep=';', dtype={'ID_Posting': str, 'ID_Posting_Parent': str, 'ID_CommunityIdentity': str, 'ID_Article': str}, parse_dates=['ArticlePublishingDate', 'PostingCreatedAt', 'UserCreatedAt']) for timeframe in ['01052019_07052019', '08052019_15052019', '16052019_23052019', '24052019_31052019']])\n",
    "votes = pd.concat([pd.read_csv(f'data/Votes_{timeframe}.csv', sep=';', dtype={'ID_CommunityIdentity': str, 'ID_Posting': str}, parse_dates=['VoteCreatedAt', 'UserCreatedAt']) for timeframe in ['01052019_07052019', '08052019_15052019', '16052019_23052019', '24052019_31052019']])\n",
    "following = pd.read_csv('data/Following_Ignoring_Relationships_01052019_31052019.csv', sep=';', dtype={'ID_CommunityIdentity': str, 'ID_CommunityIdentityConnectedTo': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to read the data stored in `data/` in order to transform it into a form that can be loaded into a Neo4j database.\n",
    "\n",
    "Neo4j supports all kinds of approaches to import data, but unfortunately, most of them are not well suited for large amounts of data like the ones we are dealing with here. The most efficient way to import data into Neo4j is to use the [neo4j-admin import command](https://neo4j.com/docs/operations-manual/current/tutorial/neo4j-admin-import/). However, it requires the data to be in a very specific format. Therefore, we have to transform our data accordingly and store it in the `graph/` directory.\n",
    "\n",
    "To load data into Neo4j, we have to create separate CSV files for nodes and edges. The nodes CSV file has to contain a header row with the column name `:ID`, which has to be globally unique. The edges CSV file has to contain a header row with the column names `:START_ID` and `:END_ID`. The `:START_ID` column contains the ID of the start node and the `:END_ID` column contains the ID of the end node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings['ID_Posting_Global'] = 'p' + postings['ID_Posting']\n",
    "postings['ID_Posting_Parent_Global'] = 'p' + postings['ID_Posting_Parent']\n",
    "postings['ID_CommunityIdentity_Global'] = 'u' + postings['ID_CommunityIdentity']\n",
    "postings['ID_Article_Global'] = 'a' + postings['ID_Article']\n",
    "votes['ID_Posting_Global'] = 'p' + votes['ID_Posting']\n",
    "votes['ID_CommunityIdentity_Global'] = 'u' + votes['ID_CommunityIdentity']\n",
    "following['ID_CommunityIdentity_Global'] = 'u' + following['ID_CommunityIdentity']\n",
    "following['ID_CommunityIdentityConnectedTo_Global'] = 'u' + following['ID_CommunityIdentityConnectedTo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, we have different types of nodes and each node contains a unique ID within its type. However, the IDs are not globally unique. Therefore, we have to create a new ID for each node that is globally unique. We can do this by concatenating the type of the node with its ID. For example, the node with the ID `1` and the type `Posting` will get the new ID `p1`. This way, we can ensure that the IDs are globally unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_nodes = postings['ID_Posting_Parent'].dropna()\n",
    "missing_nodes = missing_nodes[~missing_nodes.isin(postings['ID_Posting'])]\n",
    "missing_nodes = pd.DataFrame({'ID_Posting_Global': 'p' + missing_nodes, 'ID_Posting': missing_nodes})\n",
    "\n",
    "posting_nodes = postings[['ID_Posting_Global', 'ID_Posting', 'PostingHeadline', 'PostingComment', 'PostingCreatedAt']].copy()\n",
    "posting_nodes = pd.concat([posting_nodes, missing_nodes])\n",
    "posting_nodes.rename(columns={'ID_Posting_Global': ':ID', 'ID_Posting': 'id:long', 'PostingHeadline': 'headline', 'PostingComment': 'comment', 'PostingCreatedAt': 'created_at'}, inplace=True)\n",
    "posting_nodes[:int(len(posting_nodes) / 2)].to_csv('graph/posting_1.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "posting_nodes[int(len(posting_nodes) / 2):].to_csv('graph/posting_2.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For postings, we can find iformation about the nodes in the postings CSV file. However, we have to consider that the column `ID_Posting_Parent` can reference a posting, for which we do not have any information in the CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_nodes = pd.concat([following['ID_CommunityIdentity'].dropna(), following['ID_CommunityIdentityConnectedTo'].dropna()])\n",
    "missing_nodes = pd.DataFrame({'ID_CommunityIdentity_Global': 'u' + missing_nodes, 'ID_CommunityIdentity': missing_nodes})\n",
    "\n",
    "user_nodes = pd.concat([missing_nodes, postings[['ID_CommunityIdentity_Global', 'ID_CommunityIdentity', 'UserCommunityName', 'UserGender', 'UserCreatedAt']], votes[['ID_CommunityIdentity_Global', 'ID_CommunityIdentity', 'UserCommunityName', 'UserGender', 'UserCreatedAt']]])\n",
    "user_nodes = user_nodes.groupby('ID_CommunityIdentity').last().reset_index()\n",
    "user_nodes.rename(columns={'ID_CommunityIdentity_Global': ':ID', 'ID_CommunityIdentity': 'id:long', 'UserCommunityName': 'community_name', 'UserGender': 'gender', 'UserCreatedAt': 'created_at'}, inplace=True)\n",
    "user_nodes.to_csv('graph/user.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For users, we encounter a similar problem, where the postings and votes CSV files contains information about referenced users, but the following file does not contain any information about the users referenced. Therfore, we have to consider that there might be users that are referenced in the following CSV file, but for which we do not have any information.\n",
    "\n",
    "By concatinating all information about users and grouping it by the user ID, we can ensure that we only create one node per user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_nodes = postings[['ID_Article_Global', 'ID_Article', 'ArticlePublishingDate', 'ArticleTitle', 'ArticleChannel', 'ArticleRessortName']]\n",
    "article_nodes = article_nodes.groupby('ID_Article').last().reset_index()\n",
    "article_nodes.rename(columns={'ID_Article_Global': ':ID', 'ID_Article': 'id:long', 'ArticlePublishingDate': 'publishing_date', 'ArticleTitle': 'title', 'ArticleChannel': 'channel', 'ArticleRessortName': 'ressort'}, inplace=True)\n",
    "article_nodes.to_csv('graph/article.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about articles is only contained in the postings file. Similar to the users, we have to ensure that we only create one node per article by grouping the information by the article ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the globally unique IDs we created for the nodes, we can now create the edges CSV files by simply referencing the IDs of the nodes for each connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent Posting Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_parent_edges = postings[['ID_Posting_Global', 'ID_Posting_Parent_Global']][postings['ID_Posting_Parent_Global'].notnull()].copy()\n",
    "has_parent_edges.rename(columns={'ID_Posting_Global': ':START_ID', 'ID_Posting_Parent_Global': ':END_ID'}, inplace=True)\n",
    "has_parent_edges.to_csv('graph/has_parent.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting Article Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_on_edges = postings[['ID_Posting_Global', 'ID_Article_Global']].copy()\n",
    "posted_on_edges.rename(columns={'ID_Posting_Global': ':START_ID', 'ID_Article_Global': ':END_ID'}, inplace=True)\n",
    "posted_on_edges.to_csv('graph/posted_on.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Posting Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_by_edges = postings[['ID_Posting_Global', 'ID_CommunityIdentity_Global']].copy()\n",
    "posted_by_edges.rename(columns={'ID_Posting_Global': ':START_ID', 'ID_CommunityIdentity_Global': ':END_ID'}, inplace=True)\n",
    "posted_by_edges.to_csv('graph/posted_by.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following/Ignoring Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "follows_edges = following[following['ID_CommunityConnectionType'] == 1][['ID_CommunityIdentity_Global', 'ID_CommunityIdentityConnectedTo_Global']].copy()\n",
    "follows_edges.rename(columns={'ID_CommunityIdentity_Global': ':START_ID', 'ID_CommunityIdentityConnectedTo_Global': ':END_ID'}, inplace=True)\n",
    "follows_edges.to_csv('graph/follows.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignores_edges = following[following['ID_CommunityConnectionType'] == 2][['ID_CommunityIdentity_Global', 'ID_CommunityIdentityConnectedTo_Global']].copy()\n",
    "ignores_edges.rename(columns={'ID_CommunityIdentity_Global': ':START_ID', 'ID_CommunityIdentityConnectedTo_Global': ':END_ID'}, inplace=True)\n",
    "ignores_edges.to_csv('graph/ignores.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upvoted/Downvoted Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "upvoted_edges = votes[votes['VotePositive'] == 1][['ID_CommunityIdentity_Global', 'ID_Posting_Global', 'VoteCreatedAt']].copy()\n",
    "upvoted_edges.rename(columns={'ID_CommunityIdentity_Global': ':START_ID', 'ID_Posting_Global': ':END_ID', 'VoteCreatedAt': 'created_at'}, inplace=True)\n",
    "upvoted_edges[:int(len(upvoted_edges) / 2)].to_csv('graph/upvoted_1.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "upvoted_edges[int(len(upvoted_edges) / 2):].to_csv('graph/upvoted_2.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "downvoted_edges = votes[votes['VoteNegative'] == 1][['ID_CommunityIdentity_Global', 'ID_Posting_Global', 'VoteCreatedAt']].copy()\n",
    "downvoted_edges.rename(columns={'ID_CommunityIdentity_Global': ':START_ID', 'ID_Posting_Global': ':END_ID', 'VoteCreatedAt': 'created_at'}, inplace=True)\n",
    "downvoted_edges.to_csv('graph/downvoted.csv', index=False, date_format='%Y-%m-%dT%H:%M:%S.%fZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing\n",
    "\n",
    "In the end, we can now import the data stored in the `graph/` directory into a Neo4j database by using the following command: \n",
    "\n",
    "```{bash}\n",
    "bin/neo4j-admin database import full --nodes=Posting=import/posting_1.csv,import/posting_2.csv --nodes=User=import/user.csv --nodes=Article=import/article.csv --relationships=HAS_PARENT=import/has_parent.csv --relationships=POSTED_ON=import/posted_on.csv --relationships=POSTED_BY=import/posted_by.csv --relationships=FOLLOWS=import/follows.csv --relationships=IGNORES=import/ignores.csv --relationships=UPVOTED=import/upvoted_1.csv,import/upvoted_2.csv --relationships=DOWNVOTED=import/downvoted.csv neo4j\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
