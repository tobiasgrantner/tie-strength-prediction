{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, ensemble\n",
    "from sklearn.linear_model._base import LinearModel\n",
    "from sklearn.ensemble._forest import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from typing import List, Callable\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, explained_variance_score, max_error\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import _is_fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the different splits of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/interaction_train_set.csv', sep=',', header=0)\n",
    "df_test = pd.read_csv('data/interaction_test_set.csv', sep=',', header=0)\n",
    "df_val = pd.read_csv('data/interaction_val_set.csv', sep=',', header=0)\n",
    "\n",
    "display(df_train)\n",
    "display(df_test)\n",
    "display(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = preprocessing.scale(df_train[['reciprocity', 'multiplexity', 'closeness', 'sentiment','interactionFrequency']].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(scaled_data)\n",
    "pca_data = pca.transform(scaled_data)\n",
    "pca.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percentage of variation that each PC accounts for \n",
    "per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
    "# Create labels for Scree plot\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label=labels)\n",
    "plt.ylabel('Percentage of Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = sns.pairplot(data=df_train,\n",
    "                  y_vars=['tieStrength'],\n",
    "                  x_vars=['reciprocity', 'multiplexity', 'closeness', 'sentiment', 'interactionFrequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_params = ['reciprocity', 'multiplexity', 'closeness', 'sentiment', 'interactionFrequency']\n",
    "y_params = \"tieStrength\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train:pd.DataFrame, validation:pd.DataFrame, x:List[str], y:str, metrics:Callable, pca=None):\n",
    "    train_x = train[x].to_numpy()\n",
    "    train_y = train[y].to_numpy()\n",
    "    validation_x = validation[x].to_numpy()\n",
    "    validation_y = validation[y].to_numpy()\n",
    "\n",
    "    if pca != None:\n",
    "        train_x = pca.fit_transform(train_x) if not _is_fitted(pca) else pca.transform(train_x)\n",
    "        validation_x = pca.transform(validation_x)\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "    validation_prediction_y = model.predict(validation_x)\n",
    "    \n",
    "    results = []\n",
    "    model_name = model.__class__.__name__\n",
    "    validation_results = {\"model_name\": model_name, \"set_name\": \"validation\"}\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.__name__\n",
    "        validation_results[metric_name] = metric(validation_y, validation_prediction_y)\n",
    "    results.append(validation_results)\n",
    "    train_prediction_y = model.predict(train_x)\n",
    "    train_results = {\"model_name\": model_name, \"set_name\": \"training\"}\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.__name__\n",
    "        train_results[metric_name] = metric(train_y, train_prediction_y)\n",
    "    results.append(train_results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [linear_model.LinearRegression(),ensemble.RandomForestRegressor(), linear_model.Lasso(), linear_model.ElasticNet(),linear_model.Ridge(), linear_model.PoissonRegressor(), MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
    "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "             hidden_layer_sizes=(5,4,3,), learning_rate='constant',\n",
    "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
    "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "             power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
    "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "             warm_start=False), SVR(kernel=\"poly\", C=100, gamma=\"auto\", degree=3, epsilon=0.1, coef0=1)]\n",
    "metrics = [mean_squared_error, root_mean_squared_error, mean_absolute_error, explained_variance_score, max_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_models(model, train:pd.DataFrame, validation:pd.DataFrame, x:List[str], y:str, metrics:List[Callable], pca=None):\n",
    "    results = []\n",
    "    for model in models:\n",
    "        results.extend(train_model(model, train, validation, x, y, metrics, pca))\n",
    "    return pd.DataFrame(results)\n",
    "        \n",
    "model_results = train_models(models, df_train, df_val, x_params, y_params, metrics)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test:pd.DataFrame, x:List[str], y:str, metrics:List[Callable], pca=None):\n",
    "    test_x = test[x].to_numpy()\n",
    "    test_y = test[y].to_numpy()\n",
    "\n",
    "    if pca != None:\n",
    "        test_x = pca.transform(test_x)\n",
    "\n",
    "    predicted_y = model.predict(test_x)\n",
    "    results = []\n",
    "    model_name = model.__class__.__name__\n",
    "    test_results = {\"model_name\": model_name, \"set_name\": \"test\"}\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.__name__\n",
    "        test_results[metric_name] = metric(test_y, predicted_y)\n",
    "    results.append(test_results)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(model, test:pd.DataFrame, x:List[str], y:str, metrics:List[Callable], pca=None):\n",
    "    results = []\n",
    "    for model in models:\n",
    "        results.extend(eval_model(model, test, x, y, metrics, pca))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "eval_results = eval_models(models, df_test, x_params, y_params, metrics)\n",
    "eval_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([eval_results, model_results])\n",
    "results = results.sort_values(\"model_name\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = results.set_index([\"model_name\"])\n",
    "grouped_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped_results[['set_name', 'root_mean_squared_error']].pivot(columns='set_name').plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
