{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, ensemble\n",
    "from sklearn.linear_model._base import LinearModel\n",
    "from sklearn.ensemble._forest import RandomForestRegressor, RandomForestClassifier\n",
    "from typing import List, Callable\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, accuracy_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/interaction_train_set.csv', sep=',', header=0)\n",
    "df_test = pd.read_csv('data/interaction_test_set.csv', sep=',', header=0)\n",
    "df_val = pd.read_csv('data/interaction_val_set.csv', sep=',', header=0)\n",
    "\n",
    "display(df_train)\n",
    "display(df_test)\n",
    "display(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_params = ['reciprocity', 'multiplexity', 'closeness', 'sentiment','interactionFrequency']\n",
    "y_params = \"tieStrength\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train:pd.DataFrame, validation:pd.DataFrame, x:List[str], y:str, metrics:Callable):\n",
    "    train_x = train[x].to_numpy()\n",
    "    train_y = train[y].to_numpy()\n",
    "    model.fit(train_x, train_y)\n",
    "    validation_x = validation[x].to_numpy()\n",
    "    validation_y = validation[y].to_numpy()\n",
    "    validation_prediction_y = model.predict(validation_x)\n",
    "    results = []\n",
    "    model_name = model.__class__.__name__\n",
    "    validation_results = {\"model_name\": model_name, \"set_name\": \"validation\"}\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.__name__\n",
    "        validation_results[metric_name] = metric(validation_y, validation_prediction_y)\n",
    "    results.append(validation_results)\n",
    "    train_prediction_y = model.predict(train_x)\n",
    "    train_results = {\"model_name\": model_name, \"set_name\": \"training\"}\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.__name__\n",
    "        train_results[metric_name] = metric(train_y, train_prediction_y)\n",
    "    results.append(train_results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [linear_model.LinearRegression(),ensemble.RandomForestRegressor(), linear_model.Lasso(), linear_model.ElasticNet(),linear_model.Ridge()]\n",
    "metrics = [mean_squared_error, root_mean_squared_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_models(model, train:pd.DataFrame, validation:pd.DataFrame, x:List[str], y:str, metrics:List[Callable]):\n",
    "    results = []\n",
    "    for model in models:\n",
    "        results.extend(train_model(model, train, validation, x, y, metrics))\n",
    "    return pd.DataFrame(results)\n",
    "        \n",
    "\n",
    "model_results = train_models(models, df_train, df_val, x_params, y_params, metrics)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test:pd.DataFrame, x:List[str], y:str, metrics:List[Callable]):\n",
    "    test_x = test[x].to_numpy()\n",
    "    test_y = test[y].to_numpy()\n",
    "    predicted_y = model.predict(test_x)\n",
    "    results = []\n",
    "    model_name = model.__class__.__name__\n",
    "    test_results = {\"model_name\": model_name, \"set_name\": \"test\"}\n",
    "    for metric in metrics:\n",
    "        metric_name = metric.__name__\n",
    "        test_results[metric_name] = metric(test_y, predicted_y)\n",
    "    results.append(test_results)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(model, test:pd.DataFrame, x:List[str], y:str, metrics:List[Callable]):\n",
    "    results = []\n",
    "    for model in models:\n",
    "        results.extend(eval_model(model, test, x, y, metrics))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "eval_results = eval_models(models, df_test, x_params, y_params, metrics)\n",
    "eval_results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([eval_results, model_results])\n",
    "results = results.sort_values(\"model_name\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.set_index([\"model_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results.plot.bar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
